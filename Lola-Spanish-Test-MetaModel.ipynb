{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import json\n",
    "\n",
    "def simple_translate(texts, src_lang, tgt_lang, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Translates a list of texts using M2M100 without any entity masking.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): List of texts to translate.\n",
    "        src_lang (str): Source language code.\n",
    "        tgt_lang (str): Target language code.\n",
    "        model (M2M100ForConditionalGeneration): Pretrained M2M100 model.\n",
    "        tokenizer (M2M100Tokenizer): Tokenizer for M2M100.\n",
    "\n",
    "    Returns:\n",
    "        list of str: Translated texts.\n",
    "    \"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    translations = []\n",
    "\n",
    "    for idx, text in enumerate(texts):\n",
    "        print(f\"Translating text {idx + 1}/{len(texts)}: {text}\")\n",
    "        encoded_text = tokenizer(text, return_tensors=\"pt\")\n",
    "        generated_tokens = model.generate(\n",
    "            **encoded_text,\n",
    "            forced_bos_token_id=tokenizer.get_lang_id(tgt_lang),\n",
    "            max_length=128,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        print(f\"Translated text {idx + 1}: {translated_text}\")\n",
    "        translations.append(translated_text)\n",
    "\n",
    "    return translations\n",
    "\n",
    "# Load the M2M100 model and tokenizer\n",
    "# model_name = \"facebook/m2m100_418M\"\n",
    "m2m_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "m2m_tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# Example input data (can be replaced with actual data)\n",
    "data_path = \"data/spanish_test.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(data_path, \"r\") as file:\n",
    "    spanish_data = json.load(file)\n",
    "\n",
    "# Extract source texts (English)\n",
    "texts_to_translate = [entry[\"source\"] for entry in spanish_data]\n",
    "\n",
    "# Define source and target languages\n",
    "src_language = \"en\"\n",
    "tgt_language = \"es\"\n",
    "\n",
    "# Perform translation\n",
    "translated_texts = simple_translate(texts_to_translate, src_language, tgt_language, m2m_model, m2m_tokenizer)\n",
    "\n",
    "# Save the translated results\n",
    "results = []\n",
    "for idx, entry in enumerate(spanish_data):\n",
    "    print(f\"Processing result {idx + 1}/{len(spanish_data)}\")\n",
    "    results.append({\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"source\": entry[\"source\"],\n",
    "        \"target\": entry[\"target\"],\n",
    "        \"translated\": translated_texts[idx]\n",
    "    })\n",
    "\n",
    "output_path = \"data/meta_translation_spanish.json\"\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    json.dump(results, output_file, indent=4)\n",
    "\n",
    "print(f\"Translated results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
