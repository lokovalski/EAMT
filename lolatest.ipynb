{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install span_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from data_processing import get_subjects_by_Q_json, process_jsonl_and_json\n",
    "from span_marker import SpanMarkerModel\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "spanishfile = '/Users/lolakovalski/Desktop/School/csci375/EAMT/data/spanish_w_labels.json'\n",
    "italianfile = '/Users/lolakovalski/Desktop/School/csci375/EAMT/data/italian_w_labels.json'\n",
    "\n",
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-mbert-base-multinerd\", clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué actor protagonizó Titanic y nació en Los Ángeles, California?\n"
     ]
    }
   ],
   "source": [
    "with open(spanishfile, 'r') as jf:\n",
    "    entity_data = [json.loads(line) for line in jf]\n",
    "\n",
    "eng = entity_data[0]['source']\n",
    "esp = entity_data[0]['target']\n",
    "print(esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model.predict(eng)\n",
    "spanish_entities = model.predict(esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'span': 'Titanic', 'label': 'MEDIA', 'score': 0.9936309456825256, 'char_start_index': 23, 'char_end_index': 30}, {'span': 'Los Ángeles', 'label': 'LOC', 'score': 0.9999561309814453, 'char_start_index': 42, 'char_end_index': 53}, {'span': 'California', 'label': 'LOC', 'score': 0.999962329864502, 'char_start_index': 55, 'char_end_index': 65}]\n",
      "[{'span': 'Titanic', 'label': 'VEHI', 'score': 0.5069388151168823, 'char_start_index': 28, 'char_end_index': 35}, {'span': 'Los Angeles', 'label': 'LOC', 'score': 0.999987006187439, 'char_start_index': 52, 'char_end_index': 63}, {'span': 'California', 'label': 'LOC', 'score': 0.9999725818634033, 'char_start_index': 65, 'char_end_index': 75}]\n"
     ]
    }
   ],
   "source": [
    "print(spanish_entities)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Titanic', 'Los Angeles', 'California']\n",
      "['Titanic', 'Los Ángeles', 'California']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m([entity[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m([entity[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m spanish_entities])\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(entity_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "print([entity['span'] for entity in entities])\n",
    "print([entity['span'] for entity in spanish_entities])\n",
    "\n",
    "#print(entity_data['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Los Ángeles']\n",
      "['Los Angeles']\n",
      "['Titanic', 'Los Ángeles', 'California']\n"
     ]
    }
   ],
   "source": [
    "line = [entity_data[0]]\n",
    "lstents = [ki['entities'] for ki in line]\n",
    "\n",
    "##FROMSEMEVAL\n",
    "keys = [ent for key in lstents for ent in key]\n",
    "esents = [key[eskey]['es'] for eskey in keys for key in lstents]\n",
    "enents = [key[enkey]['en'] for enkey in keys for key in lstents]\n",
    "print(esents)\n",
    "print(enents)\n",
    "\n",
    "##FROM MODEL\n",
    "print([entity['span'] for entity in spanish_entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Titanic' 'Los Ángeles' 'California']\n"
     ]
    }
   ],
   "source": [
    "test = np.array([entity['span'] for entity in spanish_entities], dtype = object)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.80\n",
      "Recall: 0.80\n",
      "F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example data (NumPy array of lists)\n",
    "true_labels = np.array([\n",
    "    [\"cat\", \"dog\"],         # Observation 1\n",
    "    [\"mouse\"],              # Observation 2\n",
    "    [\"elephant\", \"rabbit\"]  # Observation 3\n",
    "], dtype=object)\n",
    "\n",
    "predicted_labels = np.array([\n",
    "    [\"dog\", \"cat\"],         # Prediction for Observation 1\n",
    "    [\"mouse\", \"cat\"],       # Prediction for Observation 2\n",
    "    [\"rabbit\"]              # Prediction for Observation 3\n",
    "], dtype=object)\n",
    "\n",
    "# Initialize counters\n",
    "TP, FP, FN = 0, 0, 0\n",
    "\n",
    "# Loop through each observation\n",
    "for true, pred in zip(true_labels, predicted_labels):\n",
    "    true_set = set(true)\n",
    "    pred_set = set(pred)\n",
    "    \n",
    "    TP += len(true_set & pred_set)  # Intersection: correctly predicted\n",
    "    FP += len(pred_set - true_set)  # Predicted but not true\n",
    "    FN += len(true_set - pred_set)  # True but not predicted\n",
    "\n",
    "# Calculate metrics\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
