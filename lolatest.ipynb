{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install span_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from data_processing import get_subjects_by_Q_json, process_jsonl_and_json\n",
    "from span_marker import SpanMarkerModel\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "spanishfile = '/Users/lolakovalski/Desktop/School/csci375/EAMT/data/spanish_w_labels.json'\n",
    "italianfile = '/Users/lolakovalski/Desktop/School/csci375/EAMT/data/italian_w_labels.json'\n",
    "\n",
    "model = SpanMarkerModel.from_pretrained(\"tomaarsen/span-marker-mbert-base-multinerd\", clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué actor protagonizó Titanic y nació en Los Ángeles, California?\n"
     ]
    }
   ],
   "source": [
    "with open(spanishfile, 'r') as jf:\n",
    "    entity_data = [json.loads(line) for line in jf]\n",
    "\n",
    "eng = entity_data[0]['source']\n",
    "esp = entity_data[0]['target']\n",
    "print(esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = model.predict(eng)\n",
    "spanish_entities = model.predict(esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'span': 'Titanic', 'label': 'MEDIA', 'score': 0.9936309456825256, 'char_start_index': 23, 'char_end_index': 30}, {'span': 'Los Ángeles', 'label': 'LOC', 'score': 0.9999561309814453, 'char_start_index': 42, 'char_end_index': 53}, {'span': 'California', 'label': 'LOC', 'score': 0.999962329864502, 'char_start_index': 55, 'char_end_index': 65}]\n",
      "[{'span': 'Titanic', 'label': 'VEHI', 'score': 0.5069388151168823, 'char_start_index': 28, 'char_end_index': 35}, {'span': 'Los Angeles', 'label': 'LOC', 'score': 0.999987006187439, 'char_start_index': 52, 'char_end_index': 63}, {'span': 'California', 'label': 'LOC', 'score': 0.9999725818634033, 'char_start_index': 65, 'char_end_index': 75}]\n"
     ]
    }
   ],
   "source": [
    "print(spanish_entities)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Los Ángeles']\n",
      "['Los Angeles']\n",
      "['Titanic', 'Los Ángeles', 'California']\n"
     ]
    }
   ],
   "source": [
    "line = [entity_data[0]]\n",
    "lstents = [ki['entities'] for ki in line]\n",
    "\n",
    "##FROMSEMEVAL\n",
    "keys = [ent for key in lstents for ent in key]\n",
    "esents = [key[eskey]['es'] for eskey in keys for key in lstents]\n",
    "enents = [key[enkey]['en'] for enkey in keys for key in lstents]\n",
    "print(esents)\n",
    "print(enents)\n",
    "\n",
    "##FROM MODEL\n",
    "print([entity['span'] for entity in spanish_entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Titanic' 'Los Ángeles' 'California']\n"
     ]
    }
   ],
   "source": [
    "test = np.array([entity['span'] for entity in spanish_entities], dtype = object)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.80\n",
      "Recall: 0.80\n",
      "F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array([\n",
    "    [\"cat\", \"dog\"],         # Observation 1\n",
    "    [\"mouse\"],              # Observation 2\n",
    "    [\"elephant\", \"rabbit\"]  # Observation 3\n",
    "], dtype=object)\n",
    "\n",
    "predicted_labels = np.array([\n",
    "    [\"dog\", \"cat\"],         # Prediction for Observation 1\n",
    "    [\"mouse\", \"cat\"],       # Prediction for Observation 2\n",
    "    [\"rabbit\"]              # Prediction for Observation 3\n",
    "], dtype=object)\n",
    "\n",
    "# Initialize counters\n",
    "TP, FP, FN = 0, 0, 0\n",
    "\n",
    "# Loop through each observation\n",
    "for true, pred in zip(true_labels, predicted_labels):\n",
    "    true_set = set(true)\n",
    "    pred_set = set(pred)\n",
    "    \n",
    "    TP += len(true_set & pred_set)  # Intersection: correctly predicted\n",
    "    FP += len(pred_set - true_set)  # Predicted but not true\n",
    "    FN += len(true_set - pred_set)  # True but not predicted\n",
    "\n",
    "# Calculate metrics\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '25ed319e', 'source_locale': 'en', 'target_locale': 'es', 'source': 'What is the largest park in the capital city of the UK?', 'target': '¿Cuál es el parque más grande en la capital del Reino Unido?', 'entities': {'Q145': {'es': 'Reino Unido', 'en': 'United Kingdom'}}, 'from': 'mintaka'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "with open(os.path.abspath('data/spanish_test.json'), 'r', encoding='utf-8') as jf:\n",
    "\n",
    "    inputs = json.load(jf)\n",
    "    #inputs = [json.loads(line) for line in jf]\n",
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data... \n",
      "\n",
      "Identifying named entities es... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Processing source sentences:  44%|████▍     | 133/299 [25:12<31:02, 11.22s/it] "
     ]
    }
   ],
   "source": [
    "# Import necessary functions\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from entity_masking import load_entities_mapping, mask_entities_with_spanmarker, ensure_pad_token\n",
    "from mt_translation import translate_with_placeholders_m2m\n",
    "from entity_reintegration import reintegrate_entities\n",
    "from ner_processing import sanity_check, identify_named_entities\n",
    "    \n",
    "es_train = os.path.abspath('data/spanish_test.json')\n",
    "it_train = os.path.abspath('data/italian_test.json')\n",
    "\n",
    "\n",
    "print(\"Loading data... \\n\")\n",
    "\n",
    "nerRes = {\"es\": None,\"it\": None}\n",
    "\n",
    "# Step 2: Perform Named Entity Recognition (NER)\n",
    "for language in ['es', 'it']:\n",
    "    print(\"Identifying named entities {}... \\n\".format(language))\n",
    "    file = es_train if language == 'es' else it_train\n",
    "    with open(file, 'r', encoding='utf-8') as jf:\n",
    "        inputs = json.load(jf)\n",
    "\n",
    "    nerRes[language] = identify_named_entities(inputs, language, \"test\")\n",
    "    input = {\"entities_labeled\": nerRes['target_entities_labeled'], \"entities_predicted\": nerRes['target_entities_predicted']}\n",
    "\n",
    "    precision, recall, F1 = sanity_check(input)\n",
    "    print(\"Accuracy of SpanMarker for Multilingual Named Entity Recognition {}\\n\".format(language))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"F1: {}\".format(F1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs375",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
